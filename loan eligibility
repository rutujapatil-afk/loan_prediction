# -*- coding: utf-8 -*-
"""best-loan-eligibility-prediction-da-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hGvM0FbeJH0aU2KmCSpXSB1eMQv9Wzfe

# Introduction

Welcome to our Loan Eligibility Prediction project! In this project, we're going to understand the process of getting loans and who can get them. No complicated words – just little use of machine learning algorithms.

We're looking at different things, like how much money someone makes, the amount they want to borrow, and their credit history. Imagine these factors as building blocks that fit together to decide if someone can get a loan. We're figuring out the right mix of ingredients that makes someone eligible for a loan. No rocket science, just the basics.

By the end of this project, we will know exactly what goes into the decision-making process for loan eligibility.

Let's get started.

# Dataset

**We will use two dataset:**

**Train:**

Train is used to train the model it will have all the label(answers|yes/no) assign to it. Train is going to used to develop the model and test it's accuracy.

**Test:**

The test dataset won't have any predifined label, using the model we have built we will finding the assing class and label for the text dataset.

# Data Understanding

**Attribute information:**

1. **Loan_ID:** A unique identification number assigned to each loan application.

2. **Gender:** The gender of the applicant (e.g., Male).

3. **Married:** Indicates whether the applicant is married (e.g., Yes or No).

4. **Dependents:** The number of dependents the applicant has (e.g., 0, 1).

5. **Education:** The educational qualification of the applicant (e.g., Graduate or Not Graduate).

6. **Self_Employed:** Indicates whether the applicant is self-employed (e.g., Yes or No).

7. **ApplicantIncome:** The income of the applicant.

8. **CoapplicantIncome:** The income of the co-applicant, if any.

9. **LoanAmount:** The amount of the loan applied for.

10. **Loan_Amount_Term:** The term or duration of the loan in months.

11. **Credit_History:** A binary variable indicating whether the applicant has a credit history (e.g., 1 for Yes, 0 for No).

12. **Property_Area:** The area where the property associated with the loan is located (e.g., Urban, Rural).

13. **Loan_Status:** The final status of the loan application (e.g., Y for Yes, N for No).

# Preparation
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Commented out IPython magic to ensure Python compatibility.
# Importing all the dependencies

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

# Connecting to the file

df_train = pd.read_csv("/kaggle/input/loan-eligible-dataset/loan-train.csv")
df_train.head()

"""# Exploratory Data Analysis"""

#Size and info of data

print(f"The size of the data is {df_train.shape}")
print(f"The data have  {df_train.shape[0]} rows and {df_train.shape[1]} columns.")
print()
print(f"The overall information of the data:")
print()
print(df_train.info())

# Initial Statistical Analysis
df_train.describe()

# How credit history affect the loan status
# for this we will use cross tab

pd.crosstab(df_train['Credit_History'], df_train['Loan_Status'], margins=True)

# We have to observe and visualize the applicants income

df_train.boxplot(column = 'ApplicantIncome')

df_train['ApplicantIncome'].plot(kind='hist', bins=20)

"""<font size = '4'>There are some visual outliers in Applicant Income, we will need to remove them later.</font>"""

df_train.boxplot(column='ApplicantIncome', by="Education")

_, ax = plt.subplots(ncols=2,figsize=(12,5))
df_train.boxplot(column='LoanAmount', ax=ax[0])
df_train['LoanAmount'].hist (bins=20,ax=ax[1])

"""<font size = '4'>Loan Amount is right sweked now let's normalize it!</font>"""

df_train['LoanAmount_log'] = np.log(df_train['LoanAmount'])
df_train['LoanAmount_log'].hist(bins=20)

df_train.head()

# Checking how many null vlaues in the dataset

df_train.isnull().sum()

"""<font size = '4'>There are some null values in train dataset, Let's solve that</font>"""

df_train['Gender'].fillna(df_train['Gender'].mode()[0], inplace=True)

df_train['Self_Employed'].fillna(df_train['Self_Employed'].mode()[0], inplace=True)

df_train['Married'].fillna(df_train['Married'].mode()[0], inplace=True)

df_train['Dependents'].fillna(df_train['Dependents'].mode()[0], inplace=True)

df_train['LoanAmount'].fillna(df_train['LoanAmount'].mean(), inplace=True)

df_train['LoanAmount_log'].fillna(df_train['LoanAmount_log'].mean(), inplace=True)

df_train['Loan_Amount_Term'].fillna(df_train['Loan_Amount_Term'].mode()[0], inplace=True)

df_train['Credit_History'].fillna(df_train['Credit_History'].mode()[0], inplace=True)

df_train.isnull().sum()

"""<font size = '4'>As we seen earlier, in the graphs both Applicant Income and Coapplicant Income are right skwed. Now we should fix that.</font>

<font size = '4'>Instead of normalizing the Applicant Income and Coapplicant Income separately, we can just find the total income which will be the sum of Applicant Income and Coapplicant Income</font>

"""

df_train['TotalIncome'] = df_train['ApplicantIncome'] + df_train['CoapplicantIncome']

#Visualization of total income without normalizing
df_train['TotalIncome'].hist(bins=20)

"""<font size = '4'>As we thought it is right skwed</font>

<font size = '4'>Now we have to normalize it by log
"""

df_train['TotalIncome_log'] = np.log(df_train['TotalIncome'])

# Now we should visulize if it worked or not

df_train['TotalIncome_log'].hist(bins=20)

"""<font size = '4'>**Data Preprocessing:**</font>

<font size = '4'>We've divided the dataset into independent variables (x) and the dependent variable (y), which is the 'Loan_Status.'
"""

# x have independent Variable
x = df_train.iloc[:, np.r_[1:5,9:11,13:15]].values

# y have only dependent variable
y = df_train.iloc[:, 12].values

x

y

# Importing the necessary library for implementation

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)

"""<font size = '4'>The data was split into training and testing sets.</font>"""

x_train

"""<font size = '4'>And used label encoding to convert categorical variables into numerical values."""

from sklearn.preprocessing import LabelEncoder
labelencoder_x = LabelEncoder()
labelencoder_y = LabelEncoder()

for i in range(0,5):
  x_train[:,i] = labelencoder_x.fit_transform(x_train[:,i])
x_train[:,7] = labelencoder_x.fit_transform(x_train[:,7])

x_train

y_train = labelencoder_y.fit_transform(y_train)

y_train

for i in range(0,5):
  x_test[:,i] = labelencoder_x.fit_transform(x_test[:,i])
x_test[:,7] = labelencoder_x.fit_transform(x_test[:,7])

y_test = labelencoder_y.fit_transform(y_test)

x_test

y_test

"""<font size = '4'>**Data Scaling:**

<font size = '4'>StandardScaler was applied to scale the data. Scaling is essential for certain machine learning algorithms to perform effectively, as it ensures that all features have the same scale.
"""

# Let's scale the data, by scaling the analysis become more easy

from sklearn.preprocessing import StandardScaler
ss=StandardScaler()

x_train = ss.fit_transform(x_train)
x_test = ss.fit_transform(x_test)

"""<font size = '4'>**Decision Tree Classifier:**

<font size = '4'>Applying DecisionTreeClassifier from scikit-learn using the 'entropy' criterion.
    
<font size = '4'>Training the model on the training set (x_train, y_train) and testing it on the testing set (x_test).

"""

# Applying the algorithm decision tree classifier

from sklearn.tree import DecisionTreeClassifier

dtclassifier = DecisionTreeClassifier(criterion= 'entropy', random_state=0)
dtclassifier.fit(x_train, y_train)

# Now we can use this algorithm to predict the value of test dataset
y_pred = dtclassifier.predict(x_test)
y_pred

# Let's find the accuracy

from sklearn import metrics
print("The accuracy of decision tree is: ", metrics.accuracy_score(y_pred, y_test))

"""<font size = '4'>The accuracy of the decision tree classifier on the testing set was printed using metrics.accuracy_score.

<font size = '4'>**Naive Bayes Classifier:**

<font size = '4'>Implementing the Gaussian Naive Bayes classifier using GaussianNB.
    
<font size = '4'>Similar to the decision tree, training the Naive Bayes model on the training set and predicting the outcomes on the testing set.
"""

from sklearn.naive_bayes import GaussianNB
nbclassifier = GaussianNB()

nbclassifier.fit(x_train,y_train)

y_pred = nbclassifier.predict(x_test)

y_pred

print("The accuracy of decision tree by naive bayes algorithm is: ", metrics.accuracy_score(y_pred, y_test))

"""<font size = '4'>Now let's import the test dataset, initially it has no label in it and we will use our finding in the test dataset to tell if the person is eligible for loan or not.</font>"""

df_test = pd.read_csv("/kaggle/input/loan-eligible-dataset/loan-test.csv")

df_test.head()

#Size and info of data

print(f"The size of the data is {df_test.shape}")
print(f"The data have  {df_test.shape[0]} rows and {df_test.shape[1]} columns.")
print()
print(f"The overall information of the data:")
print()
print(df_test.info())

# Checking null values if there are any
df_test.isnull().sum()

"""<font size = '4'>To remove the null values we will use the same method we used for train data</font>"""

df_test['Gender'].fillna(df_test['Gender'].mode()[0], inplace= True)
df_test['Dependents'].fillna(df_test['Dependents'].mode()[0], inplace= True)
df_test['Self_Employed'].fillna(df_test['Self_Employed'].mode()[0], inplace= True)
df_test['Loan_Amount_Term'].fillna(df_test['Loan_Amount_Term'].mean(), inplace= True)
df_test['Credit_History'].fillna(df_test['Credit_History'].mean(), inplace= True)
df_test['LoanAmount'].fillna(df_test['LoanAmount'].mean(), inplace= True)

df_test.isnull().sum()

df_test.boxplot(column='LoanAmount')

df_test.boxplot(column="ApplicantIncome")

"""<font size = '4'>We have outliers in both Loan Amount and Applicant Income, we can use log for Loan Amount and Total Income(Applicant Income + Coapplicant Income)"""

df_test['LoanAmount_log'] = np.log(df_test['LoanAmount'])

df_test['TotalIncome'] = df_test['ApplicantIncome'] + df_test['CoapplicantIncome']
df_test['TotalIncome_log'] = np.log(df_test['TotalIncome'])

df_test.head()

test = df_test.iloc[:, np.r_[1:5,9:11,13:15]].values

for i in range(0,5):
  test[:,i] = labelencoder_x.fit_transform(test[:,i])
test[:,7] = labelencoder_x.fit_transform(test[:,7])

test

test=ss.fit_transform(test)

prediction = nbclassifier.predict(test)

prediction

# Converting numerical prediction into Y/N format
predicted_labels = ['Y' if pred == 1 else 'N' for pred in prediction]

# Create a new DataFrame with Loan_ID, original features, and Predicted Loan_Status
df_predicted = df_test.copy()
df_predicted['Predicted_Loan_Status'] = predicted_labels

df_predicted['Predicted_Loan_Status_binary'] = prediction

df_predicted.head()

sns.countplot(x='Predicted_Loan_Status', data = df_predicted)
plt.title("Distribution of Predicted Loan Status")
plt.show()

numeric_columns = df_predicted.select_dtypes(include=['float64', 'int64']).columns

plt.figure(figsize=(9, 5))
sns.heatmap(df_predicted[numeric_columns].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""# Conclusion

In conclusion, the loan eligibility prediction project aimed to develop a model that could predict whether an individual is eligible for a loan based on various attributes. The analysis involved two datasets – a training set used for model development and testing, and a test set where predictions were made.

# Key Steps in the Project:
**Data Preprocessing:**

* The datasets were carefully preprocessed, including handling missing values and encoding categorical variables.
* Standard scaling was applied to ensure consistency in feature scaling.

**Model Training:**

* Two classification algorithms, Decision Tree and Naive Bayes, were employed to predict loan eligibility.
* In the scope of this data, the Naive Bayes algorithm perform well and provide good accuracy.
* The models were trained using the training dataset, and their performance was evaluated on the testing dataset.

**Prediction on Test Data:**

* The trained Naive Bayes model was applied to the test dataset to predict loan eligibility for each entry.

# Results and Findings:
* The model predictions on the test data resulted in an array ('prediction') where each element represents the model's prediction of loan eligibility.

* Unfortunately, the lack of actual outcomes for the test cases limits our ability to quantitatively assess the model's accuracy on this specific dataset.

* But we can see that the ('Credit_History') is highly correlated to the predicted loan status, and it's value also depend on TotalIncome_log as we can see in heatmap.

* We have large number of people who have been eligible for loan, we can see this in distribution of predicted loan status plot.

# Project Impact:
* While the accuracy on the test dataset remains unknown, the developed models can still be utilized for making predictions and gaining insights into potential loan eligibility.

* The project underscores the importance of having labeled data for model evaluation and highlights the need for ethical considerations in the deployment of machine learning models, especially in critical areas like financial decision-making.

**In summary, the loan eligibility prediction project has laid the groundwork for understanding the application of machine learning in predicting financial outcomes. Future iterations could build upon these foundations to enhance accuracy and applicability.**
"""
